{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Precision, Context Recall, Context Relevance and Context Entity Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have prepared two datasets manually that helps in measuring Context Precision, Context Recall, Context Relevance and Context Entity Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def load_test_set(file_path):\n",
    "    test_set = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            contexts = [row['Context']] if 'Context' in row else [row[f'Context{i}'] for i in range(1, 6) if row[f'Context{i}']]\n",
    "            test_set.append({\n",
    "                'query': row['Query'],\n",
    "                'contexts': contexts\n",
    "            })\n",
    "    return test_set\n",
    "\n",
    "def get_rag_context(query):\n",
    "    response = requests.post('http://localhost:8000/chat', json={'query': query,\"conversation_string\": \"\"})\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('movies_desc', '')\n",
    "    else:\n",
    "        print(f\"Error fetching context for query: {response}\")\n",
    "        return ''\n",
    "\n",
    "def calculate_context_precision(retrieved_context, relevant_contexts):\n",
    "    retrieved_entities = set([ent.text.lower() for ent in nlp(retrieved_context).ents])\n",
    "    relevant_entities = set([ent.text.lower() for context in relevant_contexts for ent in nlp(context).ents])\n",
    "    if not retrieved_entities:\n",
    "        return 0\n",
    "    return len(retrieved_entities.intersection(relevant_entities)) / len(retrieved_entities)\n",
    "\n",
    "def calculate_context_recall(retrieved_context, relevant_contexts):\n",
    "    retrieved_entities = set([ent.text.lower() for ent in nlp(retrieved_context).ents])\n",
    "    relevant_entities = set([ent.text.lower() for context in relevant_contexts for ent in nlp(context).ents])\n",
    "    if not relevant_entities:\n",
    "        return 1\n",
    "    return len(retrieved_entities.intersection(relevant_entities)) / len(relevant_entities)\n",
    "\n",
    "def calculate_context_relevance(query, retrieved_context):\n",
    "    vectorizer = TfidfVectorizer().fit([query, retrieved_context])\n",
    "    vectors = vectorizer.transform([query, retrieved_context])\n",
    "    return cosine_similarity(vectors[0:1], vectors[1:])[0][0]\n",
    "\n",
    "def calculate_context_entity_recall(query, retrieved_context, relevant_contexts):\n",
    "    query_entities = set([ent.text.lower() for ent in nlp(query).ents])\n",
    "    retrieved_entities = set([ent.text.lower() for ent in nlp(retrieved_context).ents])\n",
    "    relevant_entities = set([ent.text.lower() for context in relevant_contexts for ent in nlp(context).ents])\n",
    "    query_relevant_entities = query_entities.union(relevant_entities)\n",
    "    if not query_relevant_entities:\n",
    "        return 1\n",
    "    return len(retrieved_entities.intersection(query_relevant_entities)) / len(query_relevant_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_system(test_set):\n",
    "    metrics = {\n",
    "        'context_precision': [],\n",
    "        'context_recall': [],\n",
    "        'context_relevance': [],\n",
    "        'context_entity_recall': []\n",
    "    }\n",
    "\n",
    "    for test_case in test_set:\n",
    "        query = test_case['query']\n",
    "        relevant_contexts = test_case['contexts']\n",
    "        retrieved_context = get_rag_context(query)\n",
    "\n",
    "        metrics['context_precision'].append(calculate_context_precision(retrieved_context, relevant_contexts))\n",
    "        metrics['context_recall'].append(calculate_context_recall(retrieved_context, relevant_contexts))\n",
    "        metrics['context_relevance'].append(calculate_context_relevance(query, retrieved_context))\n",
    "        metrics['context_entity_recall'].append(calculate_context_entity_recall(query, retrieved_context, relevant_contexts))\n",
    "\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on single context test set...\n",
      "Results: {'context_precision': 0.7199999999999999, 'context_recall': 0.6099999999999999, 'context_relevance': 0.8000000000000002, 'context_entity_recall': 0.6499999999999998}\n"
     ]
    }
   ],
   "source": [
    "# Load both test sets\n",
    "single_context_test_set = load_test_set('testset/movie-questions.csv')\n",
    "\n",
    "# Evaluate on single context test set\n",
    "print(\"Evaluating on single context test set...\")\n",
    "single_context_results = evaluate_rag_system(single_context_test_set)\n",
    "print(\"Results:\", single_context_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on multi-context test set...\n",
      "Results: {'context_precision': 0.5999999999999999, 'context_recall': 0.55, 'context_relevance': 0.7100000000000002, 'context_entity_recall': 0.8499999999999999}\n"
     ]
    }
   ],
   "source": [
    "multi_context_test_set = load_test_set('testset/movie-recommendation.csv')\n",
    "\n",
    "# Evaluate on multi-context test set\n",
    "print(\"\\nEvaluating on multi-context test set...\")\n",
    "multi_context_results = evaluate_rag_system(multi_context_test_set)\n",
    "print(\"Results:\", multi_context_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Robustness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_response(response):\n",
    "    # List of phrases that indicate the system couldn't answer\n",
    "    unable_to_answer_phrases = [\n",
    "        \"i can't answer\",\n",
    "        \"i don't know\",\n",
    "        \"i'm not sure\",\n",
    "        \"i am unable to\",\n",
    "        \"i do not have information\",\n",
    "        \"i cannot provide\",\n",
    "        \"no information available\",\n",
    "    ]\n",
    "    \n",
    "    # Check if any of the phrases are in the response (case-insensitive)\n",
    "    return not any(phrase in response.lower() for phrase in unable_to_answer_phrases)\n",
    "def get_rag_response(query):\n",
    "    response = requests.post('http://localhost:8000/chat', json={'query': query, 'conversation_string': \"\"})\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('response', '')\n",
    "    else:\n",
    "        print(f\"Error fetching response for query: {query}\")\n",
    "        return ''\n",
    "def evaluate_noise_robustness(test_set):\n",
    "    total_queries = len(test_set)\n",
    "    valid_responses = 0\n",
    "\n",
    "    for query in test_set:\n",
    "        response = get_rag_response(query)\n",
    "        if is_valid_response(response):\n",
    "            valid_responses += 1\n",
    "        # print(f\"Query: {query}\")\n",
    "        # print(f\"Response: {response}\")\n",
    "        # print(f\"Valid: {is_valid_response(response)}\\n\")\n",
    "\n",
    "    robustness_score = (valid_responses / total_queries) * 100\n",
    "    return robustness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noise_test_set(file_path):\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        return [row['Query'] for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Robustness Score: 53.33%\n",
      "The system provided valid responses for 53.33% of the queries with faulty movie names.\n"
     ]
    }
   ],
   "source": [
    "test_set = load_noise_test_set('testset/faulty-movie-names.csv')\n",
    "robustness_score = evaluate_noise_robustness(test_set)\n",
    "print(f\"Noise Robustness Score: {robustness_score:.2f}%\")\n",
    "print(f\"The system provided valid responses for {robustness_score:.2f}% of the queries with faulty movie names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def load_test_set(file_path):\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        return list(reader)\n",
    "\n",
    "def get_rag_response(query):\n",
    "    response = requests.post('http://localhost:8000/chat', json={'query': query, 'conversation_string': \"\"})\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('response', '')\n",
    "    else:\n",
    "        print(f\"Error fetching response for query: {query}\")\n",
    "        return ''\n",
    "\n",
    "def calculate_faithfulness(response, ground_truth):\n",
    "    if not ground_truth:\n",
    "        return None\n",
    "    reference = word_tokenize(ground_truth.lower())\n",
    "    candidate = word_tokenize(response.lower())\n",
    "    return sentence_bleu([reference], candidate)\n",
    "\n",
    "def calculate_answer_relevance(query, response):\n",
    "    vectorizer = TfidfVectorizer().fit([query, response])\n",
    "    vectors = vectorizer.transform([query, response])\n",
    "    return cosine_similarity(vectors[0:1], vectors[1:])[0][0]\n",
    "\n",
    "def calculate_information_integration(response, ground_truth):\n",
    "    if not ground_truth:\n",
    "        return None\n",
    "    vectorizer = TfidfVectorizer().fit([ground_truth, response])\n",
    "    vectors = vectorizer.transform([ground_truth, response])\n",
    "    return cosine_similarity(vectors[0:1], vectors[1:])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_counterfactual_response_appropriate(response):\n",
    "    appropriate_phrases = [\n",
    "        \"hypothetical\",\n",
    "        \"imaginary scenario\",\n",
    "        \"in that case\",\n",
    "        \"if that were to happen\",\n",
    "        \"it's difficult to say\",\n",
    "        \"it's hard to predict\",\n",
    "        \"that's an interesting concept\",\n",
    "    ]\n",
    "    return any(phrase in response.lower() for phrase in appropriate_phrases)\n",
    "\n",
    "def is_negative_query_rejected(response):\n",
    "    rejection_phrases = [\n",
    "        \"I don't have enough information to answer this question.\",\n",
    "        \"i can't assist with that\",\n",
    "        \"i'm not able to help with\",\n",
    "        \"that would be illegal\",\n",
    "        \"i don't provide information about illegal activities\",\n",
    "        \"i cannot engage in or promote illegal activities\",\n",
    "        \"that's not something I can help with\",\n",
    "        \"I don't have information on illegal activities\"\n",
    "    ]\n",
    "    return any(phrase in response.lower() for phrase in rejection_phrases)\n",
    "\n",
    "def evaluate_generation_metrics(test_set):\n",
    "    metrics = {\n",
    "        'faithfulness': [],\n",
    "        'answer_relevance': [],\n",
    "        'information_integration': [],\n",
    "        'counterfactual_robustness': [],\n",
    "        'negative_rejection': []\n",
    "    }\n",
    "\n",
    "    for test_case in test_set:\n",
    "        query = test_case['Query']\n",
    "        query_type = test_case['Type']\n",
    "        ground_truth = test_case['GroundTruth']\n",
    "\n",
    "        response = get_rag_response(query)\n",
    "        \n",
    "        if query_type == 'Standard' or query_type == 'Integration':\n",
    "            faithfulness = calculate_faithfulness(response, ground_truth)\n",
    "            if faithfulness is not None:\n",
    "                metrics['faithfulness'].append(faithfulness)\n",
    "            \n",
    "            metrics['answer_relevance'].append(calculate_answer_relevance(query, response))\n",
    "            \n",
    "            if query_type == 'Integration':\n",
    "                integration = calculate_information_integration(response, ground_truth)\n",
    "                if integration is not None:\n",
    "                    metrics['information_integration'].append(integration)\n",
    "        \n",
    "        elif query_type == 'Counterfactual':\n",
    "            metrics['counterfactual_robustness'].append(is_counterfactual_response_appropriate(response))\n",
    "        \n",
    "        elif query_type == 'Negative':\n",
    "            metrics['negative_rejection'].append(is_negative_query_rejected(response))\n",
    "\n",
    "        # print(f\"Query: {query}\")\n",
    "        # print(f\"Response: {response}\")\n",
    "        # print(f\"Query Type: {query_type}\")\n",
    "        # print(\"---\")\n",
    "\n",
    "    # Calculate average scores for each metric\n",
    "    results = {}\n",
    "    for metric, scores in metrics.items():\n",
    "        if scores:\n",
    "            results[metric] = np.mean(scores)\n",
    "        else:\n",
    "            results[metric] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Metrics Results:\n",
      "faithfulness: 70.0000\n",
      "answer_relevance: 67.0000\n",
      "information_integration: 62.0000\n",
      "counterfactual_robustness: 72.0000\n",
      "negative_rejection: 60.0000\n"
     ]
    }
   ],
   "source": [
    "test_set = load_test_set('testset/generation-metrics-test-set.csv')\n",
    "results = evaluate_generation_metrics(test_set)\n",
    "    \n",
    "print(\"\\nGeneration Metrics Results:\")\n",
    "for metric, score in results.items():\n",
    "        if score is not None:\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The movie \"Titanic\" was released on November 18, 1998.\n",
      "\n",
      "Here are the details of the recommended movies:\n",
      "\n",
      "1. **Titanica**\n",
      "   - **Plot**: \"Titanica\" is a fascinating non-fiction drama that follows the 1991 expedition to the wreck of the Titanic, showcasing the adventure, drama, and danger of deep sea exploration through an international expedition team with personal interests in the legendary wreck.\n",
      "   - **Genres**: Documentary\n",
      "   - **Rating**: 6.0\n",
      "   - **Release Date**: April 1, 1995\n",
      "\n",
      "2. **Titanic**\n",
      "   - **Plot**: 84 years later, a 101-year-old woman recounts her experience on the Titanic in 1912, highlighting the love story between Rose and Jack amidst the tragic sinking of the ship.\n",
      "   - **Genres**: Drama, Romance, Thriller\n",
      "   - **Rating**: 7.5\n",
      "   - **Release Date**: November 18, 1998\n",
      "\n",
      "3. **Ghosts of the Abyss**\n",
      "   - **Plot**: Director James Cameron returns to the wreck of the Titanic with a team of experts for an unscripted adventure to the final resting place of the ship, exploring the history and tragedy of the legendary vessel.\n",
      "   - **Genres**: Documentary\n",
      "   - **Rating**: 7.1\n",
      "   - **Release Date**: April 10, 2003\n",
      "Latency: 6.6722 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_rag_response(query):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = requests.post('http://localhost:8000/chat', json={'query': query, 'conversation_string': \"\"})\n",
    "    \n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('response', ''), latency\n",
    "    else:\n",
    "        print(f\"Error fetching response for query: {query}\")\n",
    "        return '', latency\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the release date of Titanic?\"\n",
    "response, latency = get_rag_response(query)\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Latency: {latency:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
